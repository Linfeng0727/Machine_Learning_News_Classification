{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Training Journal - News Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RShmcA31dESK",
        "q8sQUW4ObXjl",
        "-uGsiQOYdESV",
        "h02G2rR4L0-X",
        "LnNOb6Wglxr1",
        "D8RpXU8adESY",
        "jd-s-LPLdESY",
        "jfw0c6CRdESZ",
        "w5NX6VOndESZ",
        "2YdP2A1JdESa",
        "Kew3JyNmdESa",
        "rG7-7DujdESa",
        "u6w4WGn0dESb",
        "JxsHWd0XzaXp",
        "rf6rdJQ60VDt",
        "oSMklBJcMMO6",
        "hMyEiuSfftC2",
        "f2gGlWotL6Jj"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deiEra8EdESI"
      },
      "source": [
        "# Model Training Journal - News Classification\n",
        "**Name:** Linfeng Liu  \n",
        "**Email:** linfeng.liu@mail.mcgill.ca \n",
        "**Kaggle:** https://www.kaggle.com/c/hw2-ycbs-273-intro-to-prac-ml/overview "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RShmcA31dESK"
      },
      "source": [
        "# 1. 12th-Aug to 14th-Aug work on bag of words model\n",
        "**Steps:** \n",
        "1. Loading and Dataset Partitioning\n",
        "2. Adding Dense layer  \n",
        "**Conclusion:**  \n",
        "1. Bag of word model can reach an accuracy of 0.91 but could not be higher.   \n",
        "2. No matter how I tune the parameter of dropout layers, the overfitting always exists.  \n",
        "**Experiment Notes:**  \n",
        "1. LayerNormalization make model converge more steadily  \n",
        "2. 'relu' and 'tanh' have almost the same impact on outcome  \n",
        "3. regularization did not work well  \n",
        "4. learning rate could be large for example 5e-4 at first, but when it comes to an approximately convergency point, we should set it smaller for example 5e-6  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3wHjfqGMnBV"
      },
      "source": [
        "## 1.1 Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHVAayjcp9ZO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJSaMc7UdESN"
      },
      "source": [
        "Download the data from Competition2 on Kaggle and import the compressed file (data_v2.zip) from it to Google Drive<br>\n",
        "as following code:<br>\n",
        "&emsp;**with zipfile.ZipFile('/content/data_v2.zip', 'r') as zip_ref:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQI6Deb0Ivzi"
      },
      "source": [
        "# Execute this only in colab after loading the 'data_v2.zip' in the workspace\n",
        "\n",
        "with zipfile.ZipFile('/content/data_v2.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_v2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeOrvPp_dESO"
      },
      "source": [
        "Dataset partitioning,using **keras.preprocessing.text_dataset_from_directory** import data from 'train' directionary.Of course, we need set the **batch_size to 512**,and **set seed to 1337(The seed is the random number seed, the purpose is to make every random number generated is fixed)**<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEVmXfOoJOFu",
        "outputId": "e8eb06d4-0899-4ed2-fa8d-8be6b240048e"
      },
      "source": [
        "# Loading the dataset from the 'train' directory\n",
        "\n",
        "batch_size = 512\n",
        "seed = 1337 # Keep the seed same for both 'train' & 'validation' to avoid overlap\n",
        "\n",
        "train_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/data_v2/train\", \n",
        "    batch_size=batch_size,\n",
        "    label_mode='int',\n",
        "    validation_split=0.3,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/data_v2/train\",\n",
        "    batch_size=batch_size,\n",
        "    label_mode='int',\n",
        "    validation_split=0.3,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 120000 files belonging to 4 classes.\n",
            "Using 84000 files for training.\n",
            "Found 120000 files belonging to 4 classes.\n",
            "Using 36000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIgAJtTLL8iz"
      },
      "source": [
        "## buffer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HEh9QPpdESQ"
      },
      "source": [
        "Create a TextVectorization instance using 2-grams and **'count'** mode. Note **'text_vectorization'** can also be used a keras layer. We will use this during the prediction on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZsjSvdlJYwx"
      },
      "source": [
        "# max_length = 50\n",
        "max_tokens = 20000\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    output_mode=\"count\",\n",
        "    max_tokens=max_tokens,\n",
        ")\n",
        "\n",
        "# Fit it on the train dataset\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "# Map the vocabulary on the 'train' and 'validation' sets\n",
        "\n",
        "count_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "count_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWVRuH81Dc30",
        "outputId": "8ec6f9ee-6d1e-484b-8118-459b90321ffe"
      },
      "source": [
        "# Printing few samples of the raw data\n",
        "\n",
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(10):\n",
        "    print(\"News: \", text_batch.numpy()[i])\n",
        "    print(\"Label:\", label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "News:  b'IBM Plans Web Meeting Service, Takes Aim at WebEx (Reuters)Reuters - IBM  plans to offer\\\\Web-conferencing as a hosted Internet service, seeking to reach\\\\small and medium-sized business customers while taking on more\\\\established rivals in the market, the company said on Tuesday.'\n",
            "Label: 3\n",
            "News:  b\"Dollar Clings to Gains Vs Euro LONDON (Reuters) - The dollar retained most of the previous  session's gains against the euro on Monday after a positive  U.S. jobs report last week reinforced expectations for an  interest rate rise later this month.\"\n",
            "Label: 2\n",
            "News:  b'History promises memorable Chennai encounterOver the years, the ground has produced some of the most endearing moments between the two countries. In the 1969-70 series, the match produced a shoot-out between two of the best post-war '\n",
            "Label: 1\n",
            "News:  b'AOL Tests Desktop Search (PC World)PC World - Upcoming browser will feature tools for finding files on your PC.'\n",
            "Label: 3\n",
            "News:  b'Belgian Grand Prix, FridayWith rain forecast for Saturday, Friday needed to be a solid, hard-working opening day at Spa; Fernando finished ninth fastest, with Jarno sixteenth.'\n",
            "Label: 1\n",
            "News:  b'Serena Fights Back, Davenport Off to a Flier in LA LOS ANGELES (Reuters) - Serena Williams came back from a  set and two breaks down to beat Anastasia Myskina 4-6, 6-3, 6-4  on the opening night of group play at the WTA Championships  Wednesday.'\n",
            "Label: 1\n",
            "News:  b'4 Die in Attack on Iraqi National Guard HQBAGHDAD, Iraq - A mortar attack on an Iraqi National Guard headquarters north of Baghdad on Tuesday killed four guardsmen and wounded 80 others, the U.S. military said...'\n",
            "Label: 0\n",
            "News:  b'Ecuador rocks but China ousted from World Cup contentionNEW YORK - Defending World Cup champion Brazil suffered the most shocking result of the day, but China was the biggest loser. The Brazilians lost to Ecuador for the just the second time in their history.'\n",
            "Label: 1\n",
            "News:  b'AMS: US actions foiling hostage dealIraq #39;s influential Association of Muslim Scholars (AMS) has accused the US army of hampering efforts to secure the release of two abducted French journalists.'\n",
            "Label: 0\n",
            "News:  b'BYE-BYE BULLIESFOR years they were the bullies of the Premiership playground but now Manchester United are finding that the little guys can fight back.'\n",
            "Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiovE87lKkIp"
      },
      "source": [
        "***Note:*** The Label of each News are well depended on the topics\n",
        "For example, label 2 is about commercial? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN0cCp4VD1X2",
        "outputId": "6ddc3570-b05b-47b7-a06c-8d350d3f426f"
      },
      "source": [
        "# Retrieve a batch (of 512 news and labels) from the dataset and printing 1 sample\n",
        "\n",
        "text_batch, label_batch = next(iter(train_ds))\n",
        "first_news, first_label = text_batch[0], label_batch[0]\n",
        "print(\"News\", first_news)\n",
        "print(\"Label\", first_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "News tf.Tensor(b'Air Force GPS Satellite Roars Into Space (AP)AP - After a series of delays, a Boeing Delta 2 rocket carrying a Global Positioning System satellite for the Air Force roared into space early Saturday.', shape=(), dtype=string)\n",
            "Label tf.Tensor(3, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYherioiDqkh"
      },
      "source": [
        "# Helper function for using 'text_vectorization'\n",
        "\n",
        "def count_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return text_vectorization(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnTOj2B8D02z",
        "outputId": "3db50d09-828a-4117-e2d6-264ec24655ab"
      },
      "source": [
        "# Printing out vectorized text data using 'text_vectorization' layer\n",
        "\n",
        "print(\"'count' vectorized question:\",\n",
        "      count_vectorize_text(first_news, first_label)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'count' vectorized question: tf.Tensor([[22.  1.  0. ...  0.  0.  0.]], shape=(1, 20000), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8sQUW4ObXjl"
      },
      "source": [
        "## 1.3 Bag of words modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0CRUkxYW7Ft"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHM09bnNdESU"
      },
      "source": [
        "**After our test, the dense layer can reduce loss to a certain extent, but once the improvement exceeds the three layers, the effect is not very significant.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlAAmExGdESU"
      },
      "source": [
        "For parameter setting, we refer to the parameters in Competition1's model, so we consider the Dense layer to set 256 neurons. Where, the optimizer is set to RMSprop and the learning rate is set to 0.005 (this is a better parameter we use in the process of tuning the model based on Competition1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FWF1Ga8h5ES"
      },
      "source": [
        "inputs = keras.Input(shape=(max_tokens,))\n",
        "x = layers.Dense(256)(inputs) # ,kernel_regularizer=regularizers.l2(0.0001)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation('relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = layers.Dense(128)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation('relu')(x)  \n",
        "x = layers.Dropout(0.6)(x)\n",
        "\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.005),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w2Lf5Pdh9AU"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                  patience=2),\n",
        "    keras.callbacks.ModelCheckpoint(\"bow_2grams_1.keras\",\n",
        "                                    save_best_only=True)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVeqFQaVia1L",
        "outputId": "e1e0f3c6-1f79-4873-ff99-b123c4026e7c"
      },
      "source": [
        "# Train the model and use validation ds for early stopping and model saving\n",
        "\n",
        "history_bow_2grams_1 = model.fit(count_train_ds,validation_data = count_val_ds, epochs=50, callbacks=callbacks,batch_size=32)\n",
        "model = keras.models.load_model(\"bow_2grams_1.keras\")\n",
        "print(f\"Test acc: {model.evaluate(count_val_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "188/188 [==============================] - 47s 239ms/step - loss: 0.3652 - accuracy: 0.8773 - val_loss: 0.2844 - val_accuracy: 0.9062\n",
            "Epoch 2/50\n",
            "188/188 [==============================] - 46s 239ms/step - loss: 0.2139 - accuracy: 0.9297 - val_loss: 0.2716 - val_accuracy: 0.9104\n",
            "Epoch 3/50\n",
            "188/188 [==============================] - 45s 234ms/step - loss: 0.1511 - accuracy: 0.9494 - val_loss: 0.3039 - val_accuracy: 0.9068\n",
            "Epoch 4/50\n",
            "188/188 [==============================] - 46s 238ms/step - loss: 0.1088 - accuracy: 0.9638 - val_loss: 0.3870 - val_accuracy: 0.8932\n",
            "47/47 [==============================] - 10s 170ms/step - loss: 0.2716 - accuracy: 0.9104\n",
            "Test acc: 0.910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uGsiQOYdESV"
      },
      "source": [
        "# 2. 17th-Aug work on Sequence modelling\n",
        "**Steps:**<br>\n",
        "&emsp;1. Sequence Modeling<br>\n",
        "&emsp;2. Adding **Embedding Layer**  \n",
        "**Conclusion:**  \n",
        "The outcome is still not good in kaggle. I tried a lot but it did not improve the prediction performance.    \n",
        "**Experiment Note:**  \n",
        "1. Adding some Dense layers has no positive impact on model, but it takes a lot of time.  \n",
        "2. Dropout is necessary, but for some reasons we can not simply use dropout to ignore overfitting.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h02G2rR4L0-X"
      },
      "source": [
        "## 2.1 TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSMNlnZUNwMd"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk1MeceLdESW"
      },
      "source": [
        "**Preparing for sequence modeling**,we use function **TextVectorization()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU9c9kZUL5SW"
      },
      "source": [
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnNOb6Wglxr1"
      },
      "source": [
        "#### Buffer \n",
        "The Buffer module is a deprecated model, so I won't go into detail here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAl9Bh6gL-o0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd3fd30-ee42-439c-fcb4-87cb9e4a94d5"
      },
      "source": [
        "'''\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "        '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass TransformerEncoder(layers.Layer):\\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\\n        super().__init__(**kwargs)\\n        self.embed_dim = embed_dim\\n        self.dense_dim = dense_dim\\n        self.num_heads = num_heads\\n        self.attention = layers.MultiHeadAttention(\\n            num_heads=num_heads, key_dim=embed_dim)\\n\\n        self.dense_proj = keras.Sequential(\\n            [layers.Dense(dense_dim, activation=\"relu\"),\\n             layers.Dense(embed_dim),]\\n        )\\n        self.layernorm_1 = layers.LayerNormalization()\\n        self.layernorm_2 = layers.LayerNormalization()\\n\\n    def call(self, inputs, mask=None):\\n        if mask is not None:\\n            mask = mask[:, tf.newaxis, :]\\n        attention_output = self.attention(\\n            inputs, inputs, attention_mask=mask)\\n        proj_input = self.layernorm_1(inputs + attention_output)\\n        proj_output = self.dense_proj(proj_input)\\n        return self.layernorm_2(proj_input + proj_output)\\n\\n    def get_config(self):\\n        config = super().get_config()\\n        config.update({\\n            \"embed_dim\": self.embed_dim,\\n            \"num_heads\": self.num_heads,\\n            \"dense_dim\": self.dense_dim,\\n        })\\n        return config\\n        '"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXmODt0FdESW"
      },
      "source": [
        "Embedding can be understood as a dimensionality reduction behavior, often translated as vectorization or vector mapping.These are very important \"basic operations\" in the whole deep learning framework. The problem of sparse input data can be solved by mapping high-dimensional data to low-dimensional space.<br>\n",
        "That's why we set both the embed_dim and dense_dim parameters here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnVoNZHoMDw8"
      },
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ7BPeNfMLRf"
      },
      "source": [
        "dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),   # With the new model, where activation= 'relu'\n",
        "             layers.Dense(embed_dim),]\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_35e9swQMNnf"
      },
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuC-5Z38dESX"
      },
      "source": [
        "## 2.2 Adding Emdedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X71kQG9cdESX"
      },
      "source": [
        "Firstly, we need to understand some basic concepts of embedding so as to provide theoretical basis for adding embedding layer. Embedding layer comes from the concept of one-hot coding. It integrates a series of texts into a sparse matrix in a specific way, and when the sparse matrix performs matrix calculation, You just multiply the numbers in the position of 1 and add them up, which is a lot easier to compute than a one-dimensional list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moAIqo4lMPEd"
      },
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "\n",
        "mask = embedding_layer.compute_mask(inputs)\n",
        "attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, \n",
        "            key_dim=embed_dim\n",
        "            )(embedded, embedded, attention_mask=mask)\n",
        "\n",
        "proj_input = layers.LayerNormalization()(embedded + attention_output)\n",
        "proj_output = dense_proj(proj_input)\n",
        "\n",
        "x = layers.LayerNormalization()(proj_input + proj_output)\n",
        "\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA6X046UMQ2S"
      },
      "source": [
        "model.compile(optimizer=RMSprop(learning_rate=5e-5),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHAdJMbTMSiE",
        "outputId": "3585a10e-1159-4071-ea33-2c3c8de62f3e"
      },
      "source": [
        "model.fit(int_train_ds,validation_data=int_val_ds,epochs=4,batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "188/188 [==============================] - 234s 1s/step - loss: 1.6492 - accuracy: 0.3841 - val_loss: 0.8354 - val_accuracy: 0.7926\n",
            "Epoch 2/4\n",
            "188/188 [==============================] - 227s 1s/step - loss: 0.8786 - accuracy: 0.6552 - val_loss: 0.5277 - val_accuracy: 0.8705\n",
            "Epoch 3/4\n",
            "188/188 [==============================] - 231s 1s/step - loss: 0.6228 - accuracy: 0.7761 - val_loss: 0.4186 - val_accuracy: 0.8848\n",
            "Epoch 4/4\n",
            "188/188 [==============================] - 235s 1s/step - loss: 0.5044 - accuracy: 0.8292 - val_loss: 0.3753 - val_accuracy: 0.8928\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe1dc3d6050>"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8BIs3yDUPQu",
        "outputId": "ea512c9f-7bdd-4528-d2a0-22a1171660e3"
      },
      "source": [
        "model.fit(int_train_ds,validation_data=int_val_ds,epochs=2,batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "188/188 [==============================] - 221s 1s/step - loss: 0.2863 - accuracy: 0.9091 - val_loss: 0.2843 - val_accuracy: 0.9089\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 240s 1s/step - loss: 0.2767 - accuracy: 0.9110 - val_loss: 0.2835 - val_accuracy: 0.9108\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe192117390>"
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8RpXU8adESY"
      },
      "source": [
        "# 3. 17th-Aug work on Model Construction with PositionalEmbedding\n",
        "**Steps:**<br>\n",
        "&emsp;1. Text Vectorization<br>\n",
        "&emsp;2. Model Construction<br>\n",
        "&emsp;3. Fitting the Model  \n",
        "**Conclusion:**  \n",
        "Just have not much improvement on prediction, also below benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd-s-LPLdESY"
      },
      "source": [
        "## 3.1 TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EHoSeNkmQjg"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "#int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfw0c6CRdESZ"
      },
      "source": [
        "## 3.2 Model Construction with PositionalEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdphATJ3megJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBnqz9fhdESZ"
      },
      "source": [
        "Still using RMSprop as optimizer for model compile, set both **dense_dim** and **embed_dim** just as the model we build before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx8SPPrSmghu"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=RMSprop(learning_rate=5e-5),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5NX6VOndESZ"
      },
      "source": [
        "## 3.3 Fitting the Model\n",
        "callback参数设置的调整问题"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbtZ_tVxmlbl",
        "outputId": "6835122e-46a3-43e6-dc13-e05ab69e126e"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "188/188 [==============================] - 233s 1s/step - loss: 2.5472 - accuracy: 0.2904 - val_loss: 1.0997 - val_accuracy: 0.6560\n",
            "Epoch 2/20\n",
            "188/188 [==============================] - 240s 1s/step - loss: 1.2234 - accuracy: 0.5026 - val_loss: 0.5901 - val_accuracy: 0.8194\n",
            "Epoch 3/20\n",
            "188/188 [==============================] - 243s 1s/step - loss: 0.6010 - accuracy: 0.7797 - val_loss: 0.3762 - val_accuracy: 0.8780\n",
            "Epoch 4/20\n",
            "188/188 [==============================] - 245s 1s/step - loss: 0.3990 - accuracy: 0.8639 - val_loss: 0.3251 - val_accuracy: 0.8953\n",
            "Epoch 5/20\n",
            "188/188 [==============================] - 244s 1s/step - loss: 0.3245 - accuracy: 0.8918 - val_loss: 0.2991 - val_accuracy: 0.9028\n",
            "Epoch 6/20\n",
            "188/188 [==============================] - 243s 1s/step - loss: 0.2798 - accuracy: 0.9073 - val_loss: 0.2790 - val_accuracy: 0.9092\n",
            "Epoch 7/20\n",
            "188/188 [==============================] - 243s 1s/step - loss: 0.2511 - accuracy: 0.9170 - val_loss: 0.2712 - val_accuracy: 0.9095\n",
            "Epoch 8/20\n",
            "188/188 [==============================] - 243s 1s/step - loss: 0.2296 - accuracy: 0.9235 - val_loss: 0.2624 - val_accuracy: 0.9125\n",
            "Epoch 9/20\n",
            "188/188 [==============================] - 243s 1s/step - loss: 0.2129 - accuracy: 0.9288 - val_loss: 0.2569 - val_accuracy: 0.9145\n",
            "Epoch 10/20\n",
            "188/188 [==============================] - 243s 1s/step - loss: 0.1984 - accuracy: 0.9334 - val_loss: 0.2612 - val_accuracy: 0.9147\n",
            "Epoch 11/20\n",
            "188/188 [==============================] - 243s 1s/step - loss: 0.1883 - accuracy: 0.9370 - val_loss: 0.2689 - val_accuracy: 0.9103\n",
            "Epoch 12/20\n",
            "188/188 [==============================] - 242s 1s/step - loss: 0.1789 - accuracy: 0.9399 - val_loss: 0.2558 - val_accuracy: 0.9145\n",
            "Epoch 13/20\n",
            "188/188 [==============================] - 242s 1s/step - loss: 0.1711 - accuracy: 0.9430 - val_loss: 0.2571 - val_accuracy: 0.9155\n",
            "Epoch 14/20\n",
            "188/188 [==============================] - 242s 1s/step - loss: 0.1628 - accuracy: 0.9451 - val_loss: 0.2578 - val_accuracy: 0.9161\n",
            "Epoch 15/20\n",
            "180/188 [===========================>..] - ETA: 9s - loss: 0.1570 - accuracy: 0.9469 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNYEJ8UK2aUy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YdP2A1JdESa"
      },
      "source": [
        "# 4. 17th-Aug Work on model with PositionEmbedding and TransformerEncoder\n",
        "**Steps:**<br>\n",
        "&emsp;1. Text Vectorization<br>\n",
        "&emsp;2. Model Construction<br>\n",
        "&emsp;3. Fitting the Model  \n",
        "**Conclusion:**  \n",
        "Still not much improvement, and it takes even more time than other modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kew3JyNmdESa"
      },
      "source": [
        "## 4.1 Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erF13hwa2d3y"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "#int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG7-7DujdESa"
      },
      "source": [
        "## 4.2 Model Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PLj7u_f2fIP"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bx-FJj123Zt"
      },
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6w4WGn0dESb"
      },
      "source": [
        "## 4.1 Fitting the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNsgWbtYdESb"
      },
      "source": [
        "层的设置和参数的设置"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL9bp86p3xGC"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuOHgArO27MV"
      },
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "#embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(x)\n",
        "#x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(128)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.005),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "0zjM3Z093f6n",
        "outputId": "18d74c57-8d3f-4ad3-96d9-a884cbc77828"
      },
      "source": [
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks,batch_size=512)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "188/188 [==============================] - 244s 1s/step - loss: 0.3652 - accuracy: 0.8726 - val_loss: 0.3975 - val_accuracy: 0.8774\n",
            "Epoch 2/20\n",
            "188/188 [==============================] - 242s 1s/step - loss: 0.2139 - accuracy: 0.9287 - val_loss: 0.2686 - val_accuracy: 0.9140\n",
            "Epoch 3/20\n",
            "188/188 [==============================] - 242s 1s/step - loss: 0.1741 - accuracy: 0.9394 - val_loss: 0.2972 - val_accuracy: 0.9092\n",
            "Epoch 4/20\n",
            "188/188 [==============================] - 242s 1s/step - loss: 0.1476 - accuracy: 0.9483 - val_loss: 0.3465 - val_accuracy: 0.9140\n",
            "Epoch 5/20\n",
            "188/188 [==============================] - 242s 1s/step - loss: 0.1292 - accuracy: 0.9546 - val_loss: 0.3174 - val_accuracy: 0.9151\n",
            "Epoch 6/20\n",
            "188/188 [==============================] - 242s 1s/step - loss: 0.1117 - accuracy: 0.9599 - val_loss: 0.4425 - val_accuracy: 0.8921\n",
            "Epoch 7/20\n",
            "188/188 [==============================] - 242s 1s/step - loss: 0.0976 - accuracy: 0.9653 - val_loss: 0.4492 - val_accuracy: 0.8978\n",
            "Epoch 8/20\n",
            "170/188 [==========================>...] - ETA: 20s - loss: 0.0872 - accuracy: 0.9690"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-55d67cfaa925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_train_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_val_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m model = keras.models.load_model(\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"full_transformer_encoder.keras\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     custom_objects={\"TransformerEncoder\": TransformerEncoder,\n\u001b[1;32m      5\u001b[0m                     \"PositionalEmbedding\": PositionalEmbedding})\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxsHWd0XzaXp"
      },
      "source": [
        "# 5. 18th-Aug work on pre-train embedding model\n",
        "**Steps:**  \n",
        "1. Data Loading(Glove)  \n",
        "2. Adding Pre-trained layer to the model  \n",
        "**Conclusion:**  \n",
        "The performance is even worse! ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKSxZHByIB36",
        "outputId": "a880f218-4631-4f0d-e2e9-a59032511d52"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-18 16:37:52--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-08-18 16:37:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-08-18 16:37:52--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.18MB/s    in 2m 40s  \n",
            "\n",
            "2021-08-18 16:40:32 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJCEye4bIH_z",
        "outputId": "eeccf35b-464a-4027-d738-c88effd10e2d"
      },
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nI42jxWINy8"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuVlY0t8IPvA"
      },
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esEzTNCROPOG"
      },
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Dropout(0.3)(embedded)\n",
        "x = layers.LayerNormalization()(x)\n",
        "x = layers.Bidirectional(layers.LSTM(32,return_sequences=True))(x)\n",
        "x = layers.LayerNormalization()(x) \n",
        "x = layers.Bidirectional(layers.LSTM(32))(x)\n",
        "x = layers.LayerNormalization()(x) \n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRk94nTiOP0I"
      },
      "source": [
        "model.compile(optimizer=RMSprop(learning_rate=8e-3),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "zcCnLgaEWwYR",
        "outputId": "22f85d55-af38-4140-ab39-75c6a7d15430"
      },
      "source": [
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=50,batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "188/188 [==============================] - 64s 275ms/step - loss: 1.2957 - accuracy: 0.4103 - val_loss: 1.0028 - val_accuracy: 0.5739\n",
            "Epoch 2/50\n",
            "188/188 [==============================] - 49s 258ms/step - loss: 0.8419 - accuracy: 0.6673 - val_loss: 0.5954 - val_accuracy: 0.7798\n",
            "Epoch 3/50\n",
            "188/188 [==============================] - 49s 259ms/step - loss: 0.6474 - accuracy: 0.7551 - val_loss: 0.5272 - val_accuracy: 0.8080\n",
            "Epoch 4/50\n",
            "188/188 [==============================] - 49s 258ms/step - loss: 0.5766 - accuracy: 0.7850 - val_loss: 0.4712 - val_accuracy: 0.8328\n",
            "Epoch 5/50\n",
            "188/188 [==============================] - 49s 259ms/step - loss: 0.5412 - accuracy: 0.7989 - val_loss: 0.4571 - val_accuracy: 0.8380\n",
            "Epoch 6/50\n",
            "188/188 [==============================] - 49s 257ms/step - loss: 0.5212 - accuracy: 0.8084 - val_loss: 0.4337 - val_accuracy: 0.8460\n",
            "Epoch 7/50\n",
            "188/188 [==============================] - 49s 258ms/step - loss: 0.5037 - accuracy: 0.8135 - val_loss: 0.4310 - val_accuracy: 0.8456\n",
            "Epoch 8/50\n",
            "188/188 [==============================] - 50s 260ms/step - loss: 0.4936 - accuracy: 0.8172 - val_loss: 0.4409 - val_accuracy: 0.8416\n",
            "Epoch 9/50\n",
            "188/188 [==============================] - 50s 262ms/step - loss: 0.4834 - accuracy: 0.8227 - val_loss: 0.4141 - val_accuracy: 0.8525\n",
            "Epoch 10/50\n",
            "188/188 [==============================] - 50s 263ms/step - loss: 0.4757 - accuracy: 0.8251 - val_loss: 0.4084 - val_accuracy: 0.8560\n",
            "Epoch 11/50\n",
            "188/188 [==============================] - 50s 262ms/step - loss: 0.4692 - accuracy: 0.8281 - val_loss: 0.4327 - val_accuracy: 0.8461\n",
            "Epoch 12/50\n",
            "188/188 [==============================] - 50s 261ms/step - loss: 0.4589 - accuracy: 0.8314 - val_loss: 0.4413 - val_accuracy: 0.8415\n",
            "Epoch 13/50\n",
            "188/188 [==============================] - 50s 260ms/step - loss: 0.4535 - accuracy: 0.8331 - val_loss: 0.3997 - val_accuracy: 0.8575\n",
            "Epoch 14/50\n",
            "188/188 [==============================] - 49s 258ms/step - loss: 0.4510 - accuracy: 0.8346 - val_loss: 0.3962 - val_accuracy: 0.8600\n",
            "Epoch 15/50\n",
            "188/188 [==============================] - 50s 260ms/step - loss: 0.4449 - accuracy: 0.8380 - val_loss: 0.4380 - val_accuracy: 0.8476\n",
            "Epoch 16/50\n",
            "188/188 [==============================] - 50s 260ms/step - loss: 0.4435 - accuracy: 0.8384 - val_loss: 0.3883 - val_accuracy: 0.8628\n",
            "Epoch 17/50\n",
            "188/188 [==============================] - 49s 258ms/step - loss: 0.4390 - accuracy: 0.8396 - val_loss: 0.3887 - val_accuracy: 0.8628\n",
            "Epoch 18/50\n",
            " 12/188 [>.............................] - ETA: 47s - loss: 0.4213 - accuracy: 0.8428"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-e9d813557e38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_train_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_val_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf6rdJQ60VDt"
      },
      "source": [
        "# 6. 18th-Aug work on BidirectionalLSTM model  \n",
        "**Steps:**  \n",
        "1. TextVectorization  \n",
        "2. Model Construction  \n",
        "**Conclusion:**  \n",
        "This is the model I spent the most of time. It works the best in all of models, but it is still lower than benchmark.  \n",
        "**Experiment Note:**  \n",
        "1. output_dim in embedded layer should be 32 (Among 16,32,64,128)  \n",
        "2. LayerNormalization is really import for model to converge steadily  \n",
        "3. The layer of Bidirectional LSTM should not be more than two. I guess the reason could be that the time sequence would lose more information if the number of layers is too large  \n",
        "4. Should not add dropout layer right below bidirectionalLSTM layer, it would have negative impact.  \n",
        "5. The number of dense layer would not have a significant impact on model prediction   \n",
        "6. Learning rate should be high at the beginning to shrink trainning time and be low to converge when it almost  reach its peak performance. But if we just want a constant learning rate, 5e-6 would be good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2qYg3Ek1TcR"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "max_length = 700\n",
        "max_tokens = 30000\n",
        "text_vectorization = layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "#int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tybGcS-aA2Er"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNALdYT81TcR"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler as LRS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_K88wKV1TcS"
      },
      "source": [
        "vocab_size = 30000\n",
        "sequence_length = 700\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "#x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "#x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "'''\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.LayerNormalization()(embedded) # A LayerNormalization is necessary here because it makes model more stable\n",
        "x = layers.Dropout(0.3)(x)\n",
        "'''\n",
        "\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=32, mask_zero=True)(inputs)\n",
        "x = layers.LayerNormalization()(embedded) # A LayerNormalization is necessary here because it makes model more stable\n",
        "x = layers.Dropout(0.3)(x)\n",
        "#embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32,return_sequences=True))(x)\n",
        "x = layers.LayerNormalization()(x)  \n",
        "#x = layers.Dropout(0.2)(x)  # According to the experiment, had better not add dropout here\n",
        "x = layers.Bidirectional(layers.LSTM(32))(x)\n",
        "x = layers.LayerNormalization()(x)\n",
        "#x = layers.Dropout(0.3)(x)   # According to the experiment, had better not add dropout here\n",
        "'''\n",
        "x = layers.Dense(256)(x) #,kernel_regularizer=regularizers.l1(0.0001)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"tanh\")(x)\n",
        "x = layers.Dropout(0.6)(x)\n",
        "'''\n",
        "x = layers.Dense(128)(x) #,kernel_regularizer=regularizers.l1(0.0001)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "lr= LRS(lambda epoch:5e-4 * 10 ** (epoch/20))\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4,momentum=0.9)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "model.compile(loss=tf.keras.losses.Huber(),optimizer=optimizer,metrics=[\"mae\"])\n",
        "\"\"\"\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_lstm.keras\",   \n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgbl85Grj3av"
      },
      "source": [
        "model.compile(optimizer=RMSprop(learning_rate=8e-4),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "xenc3BH81TcS",
        "outputId": "7e9f1a87-cd1c-4dea-c06b-4ddec23c83bb"
      },
      "source": [
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=50,batch_size=32) #, callbacks=callbacks\n",
        "#model = keras.models.load_model(\"full_transformer_encoder.keras\") #custom_objects={\"TransformerEncoder\": TransformerEncoder,\"PositionalEmbedding\": PositionalEmbedding}\n",
        "#print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "165/165 [==============================] - 100s 468ms/step - loss: 0.7541 - accuracy: 0.7157 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "Epoch 2/50\n",
            "165/165 [==============================] - 73s 436ms/step - loss: 0.3085 - accuracy: 0.8996 - val_loss: 0.3185 - val_accuracy: 0.9006\n",
            "Epoch 3/50\n",
            "165/165 [==============================] - 73s 435ms/step - loss: 0.2385 - accuracy: 0.9218 - val_loss: 0.2941 - val_accuracy: 0.9081\n",
            "Epoch 4/50\n",
            "165/165 [==============================] - 73s 436ms/step - loss: 0.1996 - accuracy: 0.9342 - val_loss: 0.2952 - val_accuracy: 0.9088\n",
            "Epoch 5/50\n",
            "165/165 [==============================] - 73s 435ms/step - loss: 0.1776 - accuracy: 0.9422 - val_loss: 0.3064 - val_accuracy: 0.9083\n",
            "Epoch 6/50\n",
            "165/165 [==============================] - 73s 435ms/step - loss: 0.1570 - accuracy: 0.9483 - val_loss: 0.3124 - val_accuracy: 0.9130\n",
            "Epoch 7/50\n",
            "165/165 [==============================] - 73s 436ms/step - loss: 0.1394 - accuracy: 0.9536 - val_loss: 0.3265 - val_accuracy: 0.9125\n",
            "Epoch 8/50\n",
            "165/165 [==============================] - 73s 437ms/step - loss: 0.1264 - accuracy: 0.9582 - val_loss: 0.3374 - val_accuracy: 0.9110\n",
            "Epoch 9/50\n",
            " 52/165 [========>.....................] - ETA: 41s - loss: 0.1156 - accuracy: 0.9611"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-09a0432fb68a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_train_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_val_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, callbacks=callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#model = keras.models.load_model(\"full_transformer_encoder.keras\") #custom_objects={\"TransformerEncoder\": TransformerEncoder,\"PositionalEmbedding\": PositionalEmbedding}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSMklBJcMMO6"
      },
      "source": [
        "# 7. 31th-Aug work on BidirectionalLSTM model with different max_length and max_tokens (Error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZqeTp2sMvnC"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeZ-1sHxMR0c"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 700  # max_length =600 before\n",
        "max_tokens = 30000  # max_tokens = 20000 before\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "#int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj0OvEaWMVKc"
      },
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=32, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=RMSprop(learning_rate=5e-5),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "_D7OwvtFMZFP",
        "outputId": "f49a878e-d5c7-4baa-e8db-1069d0cf0053"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=5, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-19b5fce3f46b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                     save_best_only=True)\n\u001b[1;32m      4\u001b[0m ]\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_train_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_val_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"one_hot_bidir_lstm.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[512,700,30000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_2/tf.one_hot_1/one_hot (defined at <ipython-input-20-19b5fce3f46b>:5) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_51906]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMyEiuSfftC2"
      },
      "source": [
        "# 8. 28th-Aug to 1st-Sep Bidirectional LSTM & Embedding Layer(glove)\n",
        "**Conclusion:** After adding the Embedding Layer, the validation loss suddenly drop from 0.26- to 0.25-, and the score in kaggle drop from 0.1414 to 0.12239.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHk6q-Af61c"
      },
      "source": [
        "# Execute this only in colab after loading the 'data_v2.zip' in the workspace\n",
        "\n",
        "with zipfile.ZipFile('/content/data_v2.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_v2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aY8T_8JuOH3"
      },
      "source": [
        "# Loading the dataset from the 'train' directory\n",
        "\n",
        "batch_size = 128\n",
        "seed = 1337 # Keep the seed same for both 'train' & 'validation' to avoid overlap\n",
        "\n",
        "train_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/data_v2/train\", \n",
        "    batch_size=batch_size,\n",
        "    label_mode='int',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/data_v2/train\",\n",
        "    batch_size=batch_size,\n",
        "    label_mode='int',\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRxPioOS5wJC"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laLLVbMs5oNM"
      },
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.LayerNormalization()(embedded)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "x = layers.LayerNormalization()(x)  \n",
        "\n",
        "x = layers.Dense(128)(x) #,kernel_regularizer=regularizers.l1(0.0001)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wUvPbSC5u-M"
      },
      "source": [
        "model.compile(optimizer=RMSprop(learning_rate=5e-5),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCD3oqQX6tOa",
        "outputId": "cb0d083f-7a0f-425b-db30-3bcdf857916a"
      },
      "source": [
        "history = model.fit(count_train_ds,validation_data = count_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"bow_2grams_1.keras\")\n",
        "print(f\"Test acc: {model.evaluate(count_val_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "750/750 [==============================] - 241s 311ms/step - loss: 0.2009 - accuracy: 0.9308 - val_loss: 0.2530 - val_accuracy: 0.9148\n",
            "Epoch 2/20\n",
            "750/750 [==============================] - 236s 312ms/step - loss: 0.1912 - accuracy: 0.9334 - val_loss: 0.2536 - val_accuracy: 0.9156\n",
            "Epoch 3/20\n",
            "750/750 [==============================] - 236s 312ms/step - loss: 0.1817 - accuracy: 0.9366 - val_loss: 0.2640 - val_accuracy: 0.9112\n",
            "188/188 [==============================] - 46s 219ms/step - loss: 0.2518 - accuracy: 0.9136\n",
            "Test acc: 0.914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2gGlWotL6Jj"
      },
      "source": [
        "# Prediction\n",
        "&emsp; **Prediction and to_csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQh3jGNtkFnD",
        "outputId": "82c49e48-cc23-4b01-f283-812777c54986"
      },
      "source": [
        "# Using the trained model to make prediction on unseen (test) data\n",
        "# Here we use the 'adapted' text_vectorization layer and include it as part of a prediction_model\n",
        "\n",
        "prediction_model = tf.keras.Sequential(\n",
        "    [text_vectorization, model])\n",
        "\n",
        "prediction_model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Test it with `val_ds`, which yields raw strings\n",
        "loss, accuracy = prediction_model.evaluate(val_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 15s 544ms/step - loss: 0.2850 - accuracy: 0.9135\n",
            "Accuracy: 91.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTrlVubEHlcb"
      },
      "source": [
        "# Read the test data in the form of a dataframe\n",
        "\n",
        "df_test_data = pd.read_csv('/content/data_v2/data_test_df.csv')\n",
        "inputs = df_test_data['data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKbRrLhQGMjT",
        "outputId": "e8441812-504a-4779-e6c5-dfcc77a7fff5"
      },
      "source": [
        "# Make sure you use the 'prediction_model' and not the trained 'model' alone\n",
        "# If you use the 'model' object, you will run int error as the data is still in the 'text' format and needs vectorization\n",
        "\n",
        "predicted_scores = prediction_model.predict(inputs)\n",
        "predicted_scores[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2.4190295e-01, 3.7546334e-04, 7.2275102e-01, 3.4970611e-02],\n",
              "       [1.3789261e-03, 3.0286697e-04, 3.4502917e-03, 9.9486792e-01],\n",
              "       [2.5791232e-03, 3.6273059e-04, 1.0878188e-02, 9.8617989e-01],\n",
              "       [1.8249109e-02, 1.1308333e-02, 9.9732224e-03, 9.6046925e-01],\n",
              "       [5.0529139e-03, 3.0256074e-04, 1.1376259e-02, 9.8326832e-01]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 19,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1HJxrvJOISFY",
        "outputId": "ace976f3-9426-4445-bb2b-8b10b65d96d6"
      },
      "source": [
        "# populating the dataframe to make a submission on Kaggle\n",
        "\n",
        "df_predictions = pd.DataFrame(predicted_scores, columns=['solution_' + str(i+1) for i in range(4)])\n",
        "df_predictions.index.rename('Id', inplace=True)\n",
        "\n",
        "df_predictions.head(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>solution_1</th>\n",
              "      <th>solution_2</th>\n",
              "      <th>solution_3</th>\n",
              "      <th>solution_4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.241903</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0.722751</td>\n",
              "      <td>0.034971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001379</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.003450</td>\n",
              "      <td>0.994868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.002579</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.010878</td>\n",
              "      <td>0.986180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.018249</td>\n",
              "      <td>0.011308</td>\n",
              "      <td>0.009973</td>\n",
              "      <td>0.960469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005053</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.011376</td>\n",
              "      <td>0.983268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.002700</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>0.014085</td>\n",
              "      <td>0.982583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.003746</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.003889</td>\n",
              "      <td>0.992101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.003598</td>\n",
              "      <td>0.000739</td>\n",
              "      <td>0.007225</td>\n",
              "      <td>0.988439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.004128</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.018604</td>\n",
              "      <td>0.977152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.023131</td>\n",
              "      <td>0.002121</td>\n",
              "      <td>0.935300</td>\n",
              "      <td>0.039448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.002451</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.078220</td>\n",
              "      <td>0.919178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.017123</td>\n",
              "      <td>0.982007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.171301</td>\n",
              "      <td>0.826415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.080666</td>\n",
              "      <td>0.001637</td>\n",
              "      <td>0.045002</td>\n",
              "      <td>0.872694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.076301</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.033913</td>\n",
              "      <td>0.889541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.005441</td>\n",
              "      <td>0.000338</td>\n",
              "      <td>0.020807</td>\n",
              "      <td>0.973414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.002841</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.009347</td>\n",
              "      <td>0.987589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.994775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.052885</td>\n",
              "      <td>0.028083</td>\n",
              "      <td>0.016668</td>\n",
              "      <td>0.902365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.002534</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>0.229340</td>\n",
              "      <td>0.767689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.021179</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.692825</td>\n",
              "      <td>0.285332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.001789</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.004412</td>\n",
              "      <td>0.993574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.001932</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.013683</td>\n",
              "      <td>0.984286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.022122</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>0.860624</td>\n",
              "      <td>0.116779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.012722</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.922300</td>\n",
              "      <td>0.064823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.006768</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.004695</td>\n",
              "      <td>0.988333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.081864</td>\n",
              "      <td>0.654488</td>\n",
              "      <td>0.259562</td>\n",
              "      <td>0.004086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.001111</td>\n",
              "      <td>0.998673</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.017874</td>\n",
              "      <td>0.980699</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.000883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.001755</td>\n",
              "      <td>0.998176</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    solution_1  solution_2  solution_3  solution_4\n",
              "Id                                                \n",
              "0     0.241903    0.000375    0.722751    0.034971\n",
              "1     0.001379    0.000303    0.003450    0.994868\n",
              "2     0.002579    0.000363    0.010878    0.986180\n",
              "3     0.018249    0.011308    0.009973    0.960469\n",
              "4     0.005053    0.000303    0.011376    0.983268\n",
              "5     0.002700    0.000632    0.014085    0.982583\n",
              "6     0.003746    0.000264    0.003889    0.992101\n",
              "7     0.003598    0.000739    0.007225    0.988439\n",
              "8     0.004128    0.000116    0.018604    0.977152\n",
              "9     0.023131    0.002121    0.935300    0.039448\n",
              "10    0.002451    0.000151    0.078220    0.919178\n",
              "11    0.000803    0.000068    0.017123    0.982007\n",
              "12    0.002200    0.000084    0.171301    0.826415\n",
              "13    0.080666    0.001637    0.045002    0.872694\n",
              "14    0.076301    0.000245    0.033913    0.889541\n",
              "15    0.005441    0.000338    0.020807    0.973414\n",
              "16    0.002841    0.000223    0.009347    0.987589\n",
              "17    0.001529    0.000187    0.003509    0.994775\n",
              "18    0.052885    0.028083    0.016668    0.902365\n",
              "19    0.002534    0.000437    0.229340    0.767689\n",
              "20    0.021179    0.000664    0.692825    0.285332\n",
              "21    0.001789    0.000225    0.004412    0.993574\n",
              "22    0.001932    0.000099    0.013683    0.984286\n",
              "23    0.022122    0.000475    0.860624    0.116779\n",
              "24    0.012722    0.000155    0.922300    0.064823\n",
              "25    0.006768    0.000203    0.004695    0.988333\n",
              "26    0.081864    0.654488    0.259562    0.004086\n",
              "27    0.001111    0.998673    0.000050    0.000166\n",
              "28    0.017874    0.980699    0.000544    0.000883\n",
              "29    0.001755    0.998176    0.000055    0.000014"
            ]
          },
          "execution_count": 20,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOclnK_xIoiq"
      },
      "source": [
        "# If using colab, then download this and submit on Kaggle\n",
        "\n",
        "df_predictions.to_csv('df_predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}